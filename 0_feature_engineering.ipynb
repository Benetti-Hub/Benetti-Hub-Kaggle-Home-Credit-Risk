{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Benetti-Hub/Benetti-Hub-Kaggle-Home-Credit-Risk/blob/main/0_feature_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qEmp1Lw3Rx3N"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0bJwJy5QOvB2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade polars\n",
        "!pip install --upgrade pandas\n",
        "!pip install lightgbm\n",
        "!pip install pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uehGbVSKOZJy",
        "outputId": "62c4e166-86dc-4409-ce83-6fa1ca037839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading home-credit-credit-risk-model-stability, 3375704785 bytes compressed\n",
            "[==================================================] 3375704785 bytes downloaded\n",
            "Downloaded and uncompressed: home-credit-credit-risk-model-stability\n",
            "Downloading credit-default-extras, 1147475 bytes compressed\n",
            "[==================================================] 1147475 bytes downloaded\n",
            "Downloaded and uncompressed: credit-default-extras\n",
            "Downloading homecredit-models-public/other/lgb/1, 16608576 bytes compressed\n",
            "[==================================================] 16608576 bytes downloaded\n",
            "Downloaded and uncompressed: homecredit-models-public/other/lgb/1\n",
            "Downloading homecredit-models-public/other/cat/1, 49048908 bytes compressed\n",
            "[==================================================] 49048908 bytes downloaded\n",
            "Downloaded and uncompressed: homecredit-models-public/other/cat/1\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = # LINK FOR THE DATASOURCES\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fd_Ro9dPOZJ2"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import os\n",
        "import pickle\n",
        "import re\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from google.colab import drive\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ai5d5ZZmKw_R",
        "outputId": "829012c7-2e59-460b-88a3-3500a70921b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "# Setup the directory for the notebook:s\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Projects/KaggleDefaults\"\n",
        "os.makedirs(DRIVE_PATH, exist_ok=True)\n",
        "\n",
        "os.chdir('/working/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 5_000_000\n",
        "\n",
        "ROOT = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
        "TRAIN_DIR = ROOT / \"parquet_files\" / \"train\"\n",
        "\n",
        "train_base = (\n",
        "    pl.read_parquet(TRAIN_DIR / \"train_base.parquet\")\n",
        "    .select(pl.col('case_id'))\n",
        "    .sort(pl.col('case_id'))\n",
        ")\n",
        "\n",
        "NUM_SAMPLES_TRAIN = train_base.shape[0]\n",
        "num_batches_train = int(np.ceil(NUM_SAMPLES_TRAIN / BATCH_SIZE))\n",
        "\n",
        "IS_ACTIVE_DICT = (\n",
        "    pd.read_csv(f\"{ROOT}/feature_definitions.csv\")\n",
        "    .assign(is_active=lambda x: x['Description'].str.lower().str.contains('active'))\n",
        "    .set_index('Variable')['is_active']\n",
        "    .to_dict()\n",
        ")\n"
      ],
      "metadata": {
        "id": "o33naojOlQkH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jf7SXpzdOZJ3"
      },
      "outputs": [],
      "source": [
        "# Utils func for the problem\n",
        "def to_pandas(df_data: pl.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Cast a polars dataframe to pandas and convert string/object columns to categorical\"\"\"\n",
        "    df_data = df_data.to_pandas()\n",
        "    cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
        "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
        "\n",
        "    return df_data\n",
        "\n",
        "\n",
        "def reduce_mem_usage(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Reduce the memory usage of a pandas dataframe\"\"\"\n",
        "\n",
        "    for col in data.columns:\n",
        "        if col == \"case_id\":\n",
        "            continue\n",
        "\n",
        "        col_type = str(data[col].dtype)\n",
        "        if col_type == \"category\":\n",
        "            continue\n",
        "        elif col_type == \"object\":\n",
        "            data[col] = data[col].astype(\"category\")\n",
        "        else:\n",
        "            c_min = data[col].min()\n",
        "            c_max = data[col].max()\n",
        "            if col_type[:3] == \"int\":\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    data[col] = data[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    data[col] = data[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    data[col] = data[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    data[col] = data[col].astype(np.int64)\n",
        "\n",
        "            elif col_type[:5] == \"float\":\n",
        "                if (\n",
        "                    c_min > np.finfo(np.float32).min\n",
        "                    and c_max < np.finfo(np.float32).max\n",
        "                ):\n",
        "                    data[col] = data[col].astype(np.float32)\n",
        "                else:\n",
        "                    data[col] = data[col].astype(np.float64)\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Hxi2-ubQOZJ3"
      },
      "outputs": [],
      "source": [
        "class Pipeline:\n",
        "    @staticmethod\n",
        "    def set_table_dtypes(data: pl.LazyFrame) -> pl.LazyFrame:\n",
        "\n",
        "        mapping = {\n",
        "            \"case_id\": pl.Int32,\n",
        "            \"date_decision\": pl.Date,\n",
        "            \"WEEK_NUM\": pl.Int16,\n",
        "            \"num_group1\": pl.Int16,\n",
        "            \"num_group2\": pl.Int16,\n",
        "            \"P\": pl.Float32,\n",
        "            \"A\": pl.Float32,\n",
        "            \"M\": pl.String,\n",
        "            \"D\": pl.Date,\n",
        "        }\n",
        "\n",
        "        for col in data.columns:\n",
        "            try:\n",
        "                if col in mapping:\n",
        "                    data = data.with_columns(\n",
        "                        pl.col(col).cast(mapping[col], strict=False)\n",
        "                    )\n",
        "                elif col[-1] in mapping:\n",
        "                    data = data.with_columns(\n",
        "                        pl.col(col).cast(mapping[col[-1]], strict=False)\n",
        "                    )\n",
        "            except:\n",
        "                to_keep = [\"target\", \"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]\n",
        "                # We cannot drop any of these columns!\n",
        "                if col not in to_keep:\n",
        "                    data = data.drop(col)\n",
        "\n",
        "        return data\n",
        "\n",
        "    @staticmethod\n",
        "    def handle_dates(data: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        data_schema = data.schema\n",
        "        for col in data.columns:\n",
        "            if col[-1] == \"D\" and isinstance(data_schema[col], pl.Datetime):\n",
        "                data = data.with_columns(pl.col(col) - pl.col(\"date_decision\"))\n",
        "                data = data.with_columns(pl.col(col).dt.total_days())\n",
        "            if \"year\" in col:\n",
        "                data = data.drop(col)\n",
        "\n",
        "        return data.drop(\"date_decision\")\n",
        "\n",
        "    @staticmethod\n",
        "    def filter_cols(data: pl.LazyFrame) -> pl.LazyFrame:\n",
        "        to_keep = [\n",
        "            \"target\",\n",
        "            \"case_id\",\n",
        "            \"WEEK_NUM\",\n",
        "            \"date_decision\",\n",
        "            \"num_group1\",\n",
        "            \"num_group2\",\n",
        "        ]\n",
        "        for col in data.columns:\n",
        "            if col not in to_keep and data[col].is_null().mean() > 0.95:\n",
        "                data = data.with_columns(pl.col(col).is_null().alias(col))\n",
        "\n",
        "        for col in data.columns:\n",
        "            if (col not in to_keep) and isinstance(data[col].dtype, pl.String):\n",
        "                freq = data[col].n_unique()\n",
        "                if (freq == 1) or (freq > 200):\n",
        "                    # Here using another type of encoding might be beneficial\n",
        "                    data = data.with_columns(pl.col(col).is_null().alias(col))\n",
        "        return data\n",
        "\n",
        "\n",
        "class Aggregator:\n",
        "\n",
        "    @staticmethod\n",
        "    def _aggregate(cols, methods):\n",
        "        return [\n",
        "            method(col).alias(f\"{method.__name__}_{col}\")\n",
        "            for method in methods\n",
        "            for col in cols\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def quantile_expr(data: pl.LazyFrame):\n",
        "\n",
        "        def build_quantile(q):\n",
        "            func = lambda x: pl.col(x).fill_null(0).quantile(q)\n",
        "            func.__name__ = f\"quantile_{q:.2f}\"\n",
        "            return func\n",
        "\n",
        "        cols = [col for col in data.columns if col[-1] in (\"P\", \"A\")]\n",
        "        agg_funcs = (build_quantile(q) for q in [0.1, 0.5, 0.75, 0.9])\n",
        "        return Aggregator._aggregate(cols, agg_funcs)\n",
        "\n",
        "    @staticmethod\n",
        "    def imq_expr(data: pl.LazyFrame):\n",
        "        def _index_mass_quantile(q) -> int:\n",
        "            def imq(x: pl.Series):\n",
        "                cumsum = pl.col(x).cum_sum()\n",
        "                distribution = cumsum / cumsum.max()\n",
        "                return pl.arg_where(\n",
        "                    (distribution >= q).cast(pl.Boolean, strict=False)\n",
        "                ).first()\n",
        "\n",
        "            func = lambda x: imq(x)\n",
        "            func.__name__ = f\"imq_{q}\"\n",
        "            return func\n",
        "\n",
        "        cols = [col for col in data.columns if col[-1] in (\"P\", \"A\")]\n",
        "        agg_funcs = (_index_mass_quantile(q) for q in [0.1, 0.75, 0.9])\n",
        "        return Aggregator._aggregate(cols, agg_funcs)\n",
        "\n",
        "    @staticmethod\n",
        "    def abs_sum_diff(data: pl.LazyFrame):\n",
        "        def _abs_sum_diff() -> int:\n",
        "            def abs_diff(x: pl.Series):\n",
        "                return pl.col(x).abs().diff().sum()\n",
        "\n",
        "            func = lambda x: abs_diff(x)\n",
        "            func.__name__ = \"abs_diff\"\n",
        "            return func\n",
        "\n",
        "        cols = [col for col in data.columns if col[-1] in (\"P\", \"A\")]\n",
        "        return Aggregator._aggregate(cols, (_abs_sum_diff(),))\n",
        "\n",
        "    @staticmethod\n",
        "    def kurtosis(data: pl.LazyFrame):\n",
        "        def _kurtosis() -> int:\n",
        "            def ks(x: pl.Series):\n",
        "                return pl.col(x).kurtosis()\n",
        "\n",
        "            func = lambda x: ks(x)\n",
        "            func.__name__ = \"ks_\"\n",
        "            return func\n",
        "\n",
        "        cols = [col for col in data.columns if col[-1] in (\"P\", \"A\")]\n",
        "        return Aggregator._aggregate(cols, (_kurtosis(),))\n",
        "\n",
        "    @staticmethod\n",
        "    def num_expr(data: pl.LazyFrame):\n",
        "        cols = [col for col in data.columns if col[-1] in (\"P\", \"A\", \"D\")]\n",
        "        agg_funcs = (pl.max, pl.min, pl.sum, pl.mean, pl.first, pl.last, pl.std)\n",
        "        return Aggregator._aggregate(cols, agg_funcs)\n",
        "\n",
        "    @staticmethod\n",
        "    def str_expr(data: pl.LazyFrame):\n",
        "        cols = [col for col in data.columns if col[-1] in (\"M\",)]\n",
        "        expr_all = Aggregator._aggregate(cols, (pl.first, pl.last, pl.max, pl.min))\n",
        "        expr_mode = [\n",
        "            pl.col(col).drop_nulls().mode().first().alias(f\"mode_{col}\") for col in cols\n",
        "        ]\n",
        "        return expr_all + expr_mode\n",
        "\n",
        "    @staticmethod\n",
        "    def other_expr(data: pl.LazyFrame):\n",
        "        cols = [col for col in data.columns if col[-1] in (\"T\", \"L\")]\n",
        "        agg_funcs = (pl.first, pl.last, pl.sum, pl.max, pl.min)\n",
        "        return Aggregator._aggregate(cols, agg_funcs)\n",
        "\n",
        "    @staticmethod\n",
        "    def count_expr(data: pl.LazyFrame):\n",
        "        cols = [col for col in data.columns if \"num_group\" in col]\n",
        "        return Aggregator._aggregate(cols, (pl.count,))\n",
        "\n",
        "    @staticmethod\n",
        "    def date_diff(data: pl.LazyFrame):\n",
        "        return data.with_columns(\n",
        "            pl.col(c)\n",
        "            .diff()\n",
        "            .over(\"case_id\")\n",
        "            .sort_by(\"num_group1\")\n",
        "            .dt.total_days()\n",
        "            .alias(f\"diff_{c}\")\n",
        "            for c in data.columns\n",
        "            if c[-1] in (\"D\")\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def full_aggregation(dataset: pl.LazyFrame):\n",
        "\n",
        "        dataset = Aggregator.date_diff(dataset)\n",
        "        return (\n",
        "            dataset.sort(pl.col(\"num_group1\"))\n",
        "            .group_by(\"case_id\")\n",
        "            .agg(\n",
        "                *Aggregator.num_expr(dataset),\n",
        "                *Aggregator.str_expr(dataset),\n",
        "                *Aggregator.other_expr(dataset),\n",
        "                *Aggregator.count_expr(dataset),\n",
        "                *Aggregator.quantile_expr(dataset),\n",
        "                *Aggregator.imq_expr(dataset),\n",
        "                *Aggregator.kurtosis(dataset),\n",
        "                *Aggregator.abs_sum_diff(dataset),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def partial_aggregation(dataset: pl.LazyFrame):\n",
        "        return (\n",
        "            dataset.sort(pl.col(\"num_group2\"))\n",
        "            .group_by(\"case_id\", \"num_group1\")\n",
        "            .agg(\n",
        "                *Aggregator.num_expr(dataset),\n",
        "                *Aggregator.str_expr(dataset),\n",
        "                *Aggregator.other_expr(dataset),\n",
        "                *Aggregator.count_expr(dataset),\n",
        "                *Aggregator.quantile_expr(dataset),\n",
        "                *Aggregator.imq_expr(dataset),\n",
        "                *Aggregator.kurtosis(dataset),\n",
        "                *Aggregator.abs_sum_diff(dataset),\n",
        "            )\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "coR5RKBxOZJ4"
      },
      "outputs": [],
      "source": [
        "def process_file(\n",
        "    path: str,\n",
        "    prefix: str,\n",
        "    p_id: int,\n",
        "    start: int,\n",
        "    stop: int,\n",
        "    keep_cols: list[str] | None = None,\n",
        "    suffix: str = \"\",\n",
        ") -> None:\n",
        "    if \"*\" in str(path):\n",
        "        partitions = glob(f\"{prefix}/{path}.parquet\")\n",
        "        data = pl.concat(\n",
        "            [pl.scan_parquet(p, low_memory=True) for p in partitions],\n",
        "            how=\"vertical_relaxed\",\n",
        "        )\n",
        "    else:\n",
        "        data = pl.scan_parquet(f\"{prefix}/{path}.parquet\", low_memory=True)\n",
        "\n",
        "    keep_cols = keep_cols or data.columns\n",
        "    data = (\n",
        "        data.select(keep_cols)\n",
        "        .pipe(Pipeline.set_table_dtypes)\n",
        "        .filter((pl.col(\"case_id\") >= start) & (pl.col(\"case_id\") < stop))\n",
        "    )\n",
        "    path = path.replace(\"_*\", \"\")\n",
        "    if path[-1] == \"1\":\n",
        "        data = data.pipe(Aggregator.full_aggregation)\n",
        "    elif path[-1] == \"2\":\n",
        "        data = data.pipe(Aggregator.partial_aggregation).pipe(\n",
        "            Aggregator.full_aggregation\n",
        "        )\n",
        "\n",
        "    save_path = f\"/kaggle/working/{path}{suffix}_{p_id}.parquet\"\n",
        "    data.collect().to_pandas().to_parquet(save_path, index=False)\n",
        "\n",
        "\n",
        "def merge_datasets(files, name, suffix: str = \"*\"):\n",
        "\n",
        "    def _filter_dataset(path):\n",
        "        (\n",
        "            pl.read_parquet(path)\n",
        "            .pipe(Pipeline.filter_cols)\n",
        "            .to_pandas()\n",
        "            .to_parquet(f\"filtered_{path}\")\n",
        "        )\n",
        "\n",
        "    base = pl.scan_parquet(\n",
        "        f\"./{files[0]}_{suffix}.parquet\", low_memory=True\n",
        "    ).with_columns(\n",
        "        month_decision=pl.col(\"date_decision\").dt.month(),\n",
        "        weekday_decision=pl.col(\"date_decision\").dt.weekday(),\n",
        "    )\n",
        "    for i, file in enumerate(files[1:]):\n",
        "        path = f\"{file}_{suffix}.parquet\"\n",
        "\n",
        "        _filter_dataset(path)\n",
        "        gc.collect()\n",
        "\n",
        "        data = pl.scan_parquet(f\"filtered_{path}\", low_memory=True)\n",
        "        base = base.join(data, how=\"left\", on=\"case_id\", suffix=f\"_{i}\", coalesce=True)\n",
        "        print(f\"joining {len(data.columns)} from {file}\")\n",
        "\n",
        "    base.pipe(Pipeline.handle_dates).collect().write_parquet(f\"{name}.parquet\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u-4KWBoXOZJ4"
      },
      "outputs": [],
      "source": [
        "sources = [\n",
        "    \"base\",\n",
        "    \"applprev_2\",\n",
        "    \"person_2\",\n",
        "    \"static_cb_0\",\n",
        "    \"static_0_*\",\n",
        "    \"other_1\",\n",
        "    \"applprev_1_*\",\n",
        "    \"tax_registry_a_1\",\n",
        "    \"tax_registry_b_1\",\n",
        "    \"tax_registry_c_1\",\n",
        "    \"credit_bureau_a_1_*\",\n",
        "    \"credit_bureau_b_1\",\n",
        "    \"person_1\",\n",
        "    \"deposit_1\",\n",
        "    \"debitcard_1\",\n",
        "    \"credit_bureau_b_2\",\n",
        "]\n",
        "\n",
        "\n",
        "# Special columns from credit_bureau_a_2_*\n",
        "common_cols = ['case_id', 'num_group1', 'num_group2']\n",
        "cba2_cols = [\n",
        "    'collater_typofvalofguarant_298M',\n",
        "    'collater_valueofguarantee_1124L',\n",
        "    'pmts_dpd_1073P',\n",
        "    'pmts_overdue_1140A',\n",
        "    'pmts_dpd_303P',\n",
        "    'pmts_overdue_1152A',\n",
        "    'subjectroles_name_541M',\n",
        "    'collaterals_typeofguarante_359M',\n",
        "    'subjectroles_name_838M',\n",
        "    'collaterals_typeofguarante_669M',\n",
        "]\n",
        "\n",
        "def preprocess_data(files, prefix, start, stop, p_id):\n",
        "\n",
        "    scan_parquet = partial(\n",
        "        process_file,\n",
        "        prefix=prefix,\n",
        "        p_id=p_id,\n",
        "        start=start,\n",
        "        stop=stop\n",
        "    )\n",
        "    for file in (pbar := tqdm(files, total=len(files))):\n",
        "        pbar.set_description(f\"Processing {file}\")\n",
        "        scan_parquet(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDEQyA5nOZJ5",
        "outputId": "0ad4f2e9-8fe5-4ee9-f3d9-70a007d2ae06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing train_credit_bureau_b_2: 100%|██████████| 16/16 [09:04<00:00, 34.03s/it]\n",
            "Processing collaterals_typeofguarante_669M: 10/10: 100%|██████████| 1/1 [16:45<00:00, 1005.04s/it]\n"
          ]
        }
      ],
      "source": [
        "train_files = [f\"train_{file}\" for file in sources]\n",
        "\n",
        "# Base preprocessing for most tables\n",
        "preprocess_data(train_files, str(TRAIN_DIR), -1, np.inf, 0)\n",
        "\n",
        "# CreditBureau2 processing:\n",
        "for batch_idx in (pbar := tqdm(range(num_batches_train), total=num_batches_train)):\n",
        "\n",
        "    start_idx = batch_idx * BATCH_SIZE\n",
        "    end_idx = min((batch_idx + 1) * BATCH_SIZE, NUM_SAMPLES_TRAIN - 1)\n",
        "\n",
        "    start = train_base[start_idx].item()\n",
        "    stop = train_base[end_idx].item()\n",
        "    if stop == train_base.max().item():\n",
        "        stop += 1  # Include right\n",
        "\n",
        "    scan_parquet = partial(\n",
        "        process_file, prefix=str(TRAIN_DIR), p_id=batch_idx, start=start, stop=stop\n",
        "    )\n",
        "    for i, col in enumerate(cba2_cols, start=1):\n",
        "        pbar.set_description(f\"Processing {col}: {i}/{len(cba2_cols)}\")\n",
        "        scan_parquet(\n",
        "            path=\"train_credit_bureau_a_2_*\",\n",
        "            keep_cols=common_cols + [col],\n",
        "            suffix=f\"_{col}\",\n",
        "        )\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "total_files = [\n",
        "    *[f.replace(\"_*\", \"\") for f in train_files],\n",
        "    *[f\"train_credit_bureau_a_2_{col}\" for col in cba2_cols],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bDFimHQ-g5H6"
      },
      "outputs": [],
      "source": [
        "weeks_filter = (pl.col('WEEK_NUM') <= 62) | (pl.col('WEEK_NUM') >= 71)\n",
        "train_data = (\n",
        "    pl.scan_parquet('/kaggle/working/train_data.parquet')\n",
        "    .filter(weeks_filter)\n",
        "    .collect()\n",
        "    .pipe(to_pandas)\n",
        "    .pipe(reduce_mem_usage)\n",
        ")\n",
        "train_data.to_parquet(f\"{DRIVE_PATH}/train_data_pandas.parquet\", index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 7921029,
          "sourceId": 50160,
          "sourceType": "competition"
        },
        {
          "datasetId": 4617924,
          "sourceId": 7976338,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}